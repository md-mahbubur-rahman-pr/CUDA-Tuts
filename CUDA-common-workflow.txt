Both CPUs and GPUs are separate entities; they have their own memory space. CPU cannot directly access GPU memory, and vice versa. 
In CUDA terminology, 
-- CPU memory is called host memory and GPU memory is called device memory. 
-- Pointers to CPU memory are called host pointer and pointers to GPU memory are called device pointer.

If GPU need to work with data, it must be presented in the device memory. 
CUDA provides APIs for allocating and deallocating/freeing device memory,
also provides APIs for transfering data between host and device memory. 

Common workflow of CUDA program looks like as following:

-- Allocate host memory and initialize host data 
-- Allocate device memory
-- Transfer input data from host memory to device memory
-- Execute kernels
-- Transfer output from device memory to host memory
